{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概述\n",
    "决策树（Decision Tree）算法是一种基本的分类与回归方法，这章节只讨论用于分类的决策树。\n",
    "\n",
    "决策树模型呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程。它可以认为是 if-then 规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。\n",
    "\n",
    "决策树学习通常包括 3 个步骤: 特征选择、决策树的生成和决策树的修剪。\n",
    "\n",
    "## 定义\n",
    "决策树由结点（node）和有向边（directed edge）组成。结点有两种类型: 内部结点（internal node）和叶结点（leaf node）。内部结点表示一个特征或属性(features)，叶结点表示一个类(labels)。\n",
    "\n",
    "用决策树对需要测试的实例进行分类: \n",
    "-   从根节点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子结点；\n",
    "-   这时，每一个子结点对应着该特征的一个取值。\n",
    "-   递归地对实例进行测试并分配，直至达到叶结点。最后将实例分配到叶结点的类。\n",
    "\n",
    "## 原理\n",
    "### 信息熵 & 信息增益\n",
    "-   信息论（information theory）中的熵（香农熵）: 是一种信息的度量方式，表示信息的混乱程度，也就是说: 信息越有序，信息熵越低；信息越无序，信息熵越高\n",
    "-   信息增益（information gain）: 在划分数据集前后信息发生的变化称为信息增益。\n",
    "\n",
    "## 算法特点\n",
    "-   优点: 计算复杂度不高，输出结果易于理解，数据有缺失也能跑，可以处理不相关特征。\n",
    "-   缺点: 容易过拟合。\n",
    "-   适用数据类型: 数值型和标称型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实战1\n",
    "## 概述\n",
    "根据以下 2 个特征，将动物分成两类: 鱼类和非鱼类。\n",
    "-   不浮出水面是否可以生存\n",
    "-   是否有脚蹼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from math import log\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataSet():\n",
    "    dataSet = [[1, 1, 'yes'],\n",
    "            [1, 1, 'yes'],\n",
    "            [1, 0, 'no'],\n",
    "            [0, 1, 'no'],\n",
    "            [0, 1, 'no']]\n",
    "    labels = ['no surfacing', 'flippers']\n",
    "    return dataSet, labels\n",
    "\n",
    "# 计算给定数据集的香农熵\n",
    "# H = -Σp(x)log2p(x), x属于类别集合\n",
    "def calShannonEnt(dataSet):\n",
    "    numEntries = len(dataSet)\n",
    "    labelCounts = {}\n",
    "    for featVec in dataSet:\n",
    "        currentLabel = featVec[-1]\n",
    "        # 为所有可能的分类创建字典\n",
    "        if currentLabel not in labelCounts.keys():\n",
    "            labelCounts[currentLabel] = 0\n",
    "        labelCounts[currentLabel] += 1\n",
    "\n",
    "    shannonEnt = 0.0\n",
    "    for key in labelCounts:\n",
    "        prob = float(labelCounts[key]) / numEntries\n",
    "        shannonEnt -= prob * log(prob, 2)\n",
    "\n",
    "    return shannonEnt\n",
    "\n",
    "# 按照给定特征划分数据集\n",
    "def splitDataSet(dataSet, index, value):\n",
    "    retDataSet = []\n",
    "    for featVec in dataSet:\n",
    "        # 找到指定特征列为value的数据，将其余特征列组成新的数据集\n",
    "        if featVec[index] == value:\n",
    "            reducedFeatVec = featVec[:index]\n",
    "            '''\n",
    "            extend和append的区别:\n",
    "            music_media.append(object) 向列表中添加一个对象object\n",
    "            music_media.extend(sequence) 把一个序列seq的内容添加到列表中 (跟 += 在list运用类似， music_media += sequence)\n",
    "            1、使用append的时候，是将object看作一个对象，整体打包添加到music_media对象中。\n",
    "            2、使用extend的时候，是将sequence看作一个序列，将这个序列和music_media序列合并，并放在其后面。\n",
    "            music_media = []\n",
    "            music_media.extend([1,2,3])\n",
    "            print music_media\n",
    "            #结果: \n",
    "            #[1, 2, 3]\n",
    "            \n",
    "            music_media.append([4,5,6])\n",
    "            print music_media\n",
    "            #结果: \n",
    "            #[1, 2, 3, [4, 5, 6]]\n",
    "            \n",
    "            music_media.extend([7,8,9])\n",
    "            print music_media\n",
    "            #结果: \n",
    "            #[1, 2, 3, [4, 5, 6], 7, 8, 9]\n",
    "            '''\n",
    "            reducedFeatVec.extend(featVec[index+1:])\n",
    "            retDataSet.append(reducedFeatVec)\n",
    "    return retDataSet\n",
    "\n",
    "# 选择最好的数据集划分方式\n",
    "def chooseBestFeatureToSplit(dataSet):\n",
    "    numFeatures = len(dataSet[0]) - 1\n",
    "    # 数据集的原始信息熵\n",
    "    baseEntropy = calShannonEnt(dataSet)\n",
    "    # 最优的信息增益值, 和最优的Feature编号\n",
    "    baseInfoGain, bestFeature = 0.0, -1\n",
    "\n",
    "    for i in range(numFeatures):\n",
    "        # 获取第i个特征的所有可能取值\n",
    "        featList = [example[i] for example in dataSet]\n",
    "        # 去重\n",
    "        uniqueVals = set(featList)\n",
    "\n",
    "        newEntropy = 0.0\n",
    "        # 对每个特征值划分数据集，计算信息熵\n",
    "        for value in uniqueVals:\n",
    "            subDataSet = splitDataSet(dataSet, i, value)\n",
    "            prob = len(subDataSet) / float(len(dataSet))\n",
    "            newEntropy += prob * calShannonEnt(subDataSet)\n",
    "\n",
    "        infoGain = baseEntropy - newEntropy\n",
    "        if infoGain > baseInfoGain:\n",
    "            baseInfoGain = infoGain\n",
    "            bestFeature = i\n",
    "    return bestFeature\n",
    "\n",
    "# 辅助函数，返回出现次数最多的类别\n",
    "def majorityCnt(classList):\n",
    "\n",
    "# 创建树\n",
    "def createTree(dataSet, labels):\n",
    "    classList = [example[-1] for example in dataSet]\n",
    "    # 停止条件1：所有类标签完全相同\n",
    "    if classList.count(classList[0]) == len(classList):\n",
    "        return classList[0]\n",
    "    \n",
    "    # 停止条件2：使用完了所有特征, 返回出现次数最多的类标签\n",
    "    if len(dataSet) == 1:\n",
    "        return majorityCnt(classList)\n",
    "\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Violet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
