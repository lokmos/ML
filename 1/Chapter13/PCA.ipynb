{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 降维技术\n",
    "## 1. 主成分分析 (Principal Component Analysis, PCA)\n",
    "\n",
    "### 核心思想\n",
    "- PCA 的目的是在保持数据方差尽可能多的前提下，将高维数据投影到低维空间。\n",
    "- 通过线性变换找到数据的主成分（Principal Components），这些主成分是原始特征的线性组合且彼此正交（即线性无关）。\n",
    "\n",
    "### 关键步骤\n",
    "1. 对数据进行标准化处理（使每个特征的均值为 0，方差为 1）。\n",
    "2. 计算协方差矩阵（衡量特征之间的关系）。\n",
    "3. 计算协方差矩阵的特征值和特征向量。\n",
    "4. 选择最大特征值对应的特征向量作为主成分，构建新的坐标系。\n",
    "5. 用这些主成分对数据进行降维。\n",
    "\n",
    "### 通俗理解\n",
    "PCA 就是找一组新的轴（特征方向），这些轴是最能解释数据变化的方向，保留最重要的几个轴进行降维。例如，考察智力情况时，可能只需要看数学成绩，而忽略其他不太重要的特征。\n",
    "\n",
    "### 优点\n",
    "- 高效，适用于大规模数据集。\n",
    "- 不需要先验知识。\n",
    "- 易于实现，结果可解释。\n",
    "\n",
    "### 局限性\n",
    "- 仅考虑线性关系，忽略非线性模式。\n",
    "- 对数据的缩放和中心化敏感。\n",
    "- 无法处理缺失值。\n",
    "\n",
    "### 应用场景\n",
    "图像处理、压缩、金融数据分析、特征工程等。\n",
    "\n",
    "---\n",
    "\n",
    "## 2. 因子分析 (Factor Analysis, FA)\n",
    "\n",
    "### 核心思想\n",
    "- 因子分析假设观察数据是由少数几个隐变量（潜在因子）和噪声的线性组合产生的。\n",
    "- 这些隐变量通常不可直接观测，但通过因子分析模型可以估计隐变量的值。\n",
    "\n",
    "### 关键步骤\n",
    "1. 定义因子模型：假设每个观测变量是若干因子和噪声的线性组合。\n",
    "2. 使用协方差矩阵分析变量之间的关系，提取潜在因子。\n",
    "3. 对提取的因子进行旋转（如正交旋转或斜交旋转），使其具有更好的可解释性。\n",
    "4. 评估模型的拟合优度，确定因子数目。\n",
    "\n",
    "### 通俗理解\n",
    "假如我们考察一个学生的综合能力，与其分别分析语文、数学和英语成绩，不如将这三个科目组合成一个“学习能力”因子。这种聚合方式减少了分析的复杂性。\n",
    "\n",
    "### 优点\n",
    "- 能处理噪声数据。\n",
    "- 适用于多个相关变量的降维和聚合。\n",
    "- 可解释性强，因子能直接对应实际问题中的潜在特征。\n",
    "\n",
    "### 局限性\n",
    "- 对数据分布有较强的假设要求（如正态分布）。\n",
    "- 模型复杂，参数估计过程较繁琐。\n",
    "\n",
    "### 应用场景\n",
    "社会科学、心理学（如性格测量）、金融（如市场风险因子分析）。\n",
    "\n",
    "---\n",
    "\n",
    "## 3. 独立成分分析 (Independent Component Analysis, ICA)\n",
    "\n",
    "### 核心思想\n",
    "- ICA 假设观测数据是若干统计上 **相互独立** 的信号线性混合的结果。\n",
    "- ICA 的目标是从混合信号中分离出这些独立信号（解混）。\n",
    "\n",
    "### 关键步骤\n",
    "1. 数据预处理，包括去中心化和白化（使数据去除冗余相关性）。\n",
    "2. 使用最大化非高斯性的方法找到最独立的信号源。\n",
    "3. 还原信号源，得到独立分量。\n",
    "\n",
    "### 通俗理解\n",
    "想象在 KTV 房间中，有两种声音混在一起：一个是原唱，另一个是主唱。ICA 会分离出这两种声音，帮你分别听清楚是哪首歌和谁在唱。\n",
    "\n",
    "### 优点\n",
    "- 能捕获高阶统计信息。\n",
    "- 擅长信号分离，例如语音和图像处理。\n",
    "- 对于混合信号处理非常有效。\n",
    "\n",
    "### 局限性\n",
    "- 假设信号之间是独立的，可能不符合实际情况。\n",
    "- 需要较大的样本量，性能受噪声干扰影响较大。\n",
    "\n",
    "### 应用场景\n",
    "语音分离（如盲源分离问题）、图像分割、脑电信号处理（EEG 数据分析）。\n",
    "\n",
    "---\n",
    "\n",
    "## 为什么 PCA 使用最广？\n",
    "\n",
    "1. **简单高效**：\n",
    "   - PCA 计算快速，适合大规模数据集。\n",
    "   - 特别适合线性相关的高维数据集降维。\n",
    "   - 在特征工程中常用于减少数据维度以提高机器学习模型的效率。\n",
    "\n",
    "2. **无强分布假设**：\n",
    "   - 与因子分析（需要数据符合正态分布）相比，PCA 对数据分布没有过多限制。\n",
    "\n",
    "3. **明确目标**：\n",
    "   - PCA 的目标是最大化数据的方差，使降维后的数据保留尽可能多的信息量。\n",
    "   - 这一目标简单且适用于多种场景。\n",
    "\n",
    "4. **应用广泛**：\n",
    "   - 图像压缩：PCA 被用来去除冗余信息，减少存储空间。\n",
    "   - 数据可视化：将高维数据降至 2D 或 3D，便于可视化分析。\n",
    "   - 特征选择：通过降维去除不重要的特征，简化模型。\n",
    "\n",
    "5. **工具和库支持**：\n",
    "   - PCA 已被集成到各种主流的数据分析工具（如 Python 的 sklearn、R 等），实现简单，使用便捷。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA算法流程\n",
    "## 1. 数据预处理\n",
    "\n",
    "### 步骤：\n",
    "- 将数据标准化，确保不同特征具有相同的量纲。\n",
    "- 公式为：\n",
    "\n",
    "$$\n",
    "x_{i,j}^{\\text{standardized}} = \\frac{x_{i,j} - \\mu_j}{\\sigma_j}\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- $x_{i,j}$ 表示数据集中第 $i$ 个样本的第 $j$ 个特征值。\n",
    "- $\\mu_j$ 为第 $j$ 个特征的均值。\n",
    "- $\\sigma_j$ 为第 $j$ 个特征的标准差。\n",
    "\n",
    "---\n",
    "\n",
    "## 2. 构建协方差矩阵\n",
    "\n",
    "### 步骤：\n",
    "- 计算标准化后的数据集的协方差矩阵，用于衡量各特征之间的线性关系。\n",
    "- 公式为：\n",
    "\n",
    "$$\n",
    "\\mathbf{C} = \\frac{1}{n-1} \\mathbf{X}^\\top \\mathbf{X}\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- $\\mathbf{X}$ 是标准化后的数据矩阵，大小为 $n \\times d$（$n$ 为样本数，$d$ 为特征数）。\n",
    "- $\\mathbf{C}$ 为大小为 $d \\times d$ 的协方差矩阵。\n",
    "\n",
    "协方差矩阵的每个元素定义为：\n",
    "\n",
    "$$\n",
    "\\text{Cov}(X_j, X_k) = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_{i,j} - \\mu_j)(x_{i,k} - \\mu_k)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3. 计算特征值和特征向量\n",
    "\n",
    "### 步骤：\n",
    "- 对协方差矩阵 $\\mathbf{C}$ 求解特征值和特征向量。\n",
    "- 特征值公式：\n",
    "\n",
    "$$\n",
    "\\mathbf{C} \\mathbf{v} = \\lambda \\mathbf{v}\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- $\\lambda$ 为特征值，表示该方向上数据的方差大小。\n",
    "- $\\mathbf{v}$ 为特征向量，表示数据投影的方向。\n",
    "\n",
    "### 解释：\n",
    "- 特征值越大，对应的特征向量越重要，表示该方向的数据变化越大。\n",
    "\n",
    "---\n",
    "\n",
    "## 4. 选择主成分\n",
    "\n",
    "### 步骤：\n",
    "- 将特征值按降序排列，选择前 $k$ 个最大的特征值及其对应的特征向量，构成投影矩阵：\n",
    "\n",
    "$$\n",
    "\\mathbf{W} = [\\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_k]\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- $k$ 是降维后的目标维度。\n",
    "- $\\mathbf{W}$ 是大小为 $d \\times k$ 的投影矩阵。\n",
    "\n",
    "### 选择主成分的依据：\n",
    "- **方差贡献率**：衡量主成分对总方差的贡献，公式为：\n",
    "\n",
    "$$\n",
    "\\text{方差贡献率} = \\frac{\\lambda_i}{\\sum_{j=1}^{d} \\lambda_j}\n",
    "$$\n",
    "\n",
    "- 通常选择方差贡献率累计达到 90% 或 95% 的前 $k$ 个主成分。\n",
    "\n",
    "---\n",
    "\n",
    "## 5. 数据投影\n",
    "\n",
    "### 步骤：\n",
    "- 将原始数据投影到低维空间，公式为：\n",
    "\n",
    "$$\n",
    "\\mathbf{Z} = \\mathbf{X} \\mathbf{W}\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- $\\mathbf{X}$ 为标准化后的原始数据矩阵，大小为 $n \\times d$。\n",
    "- $\\mathbf{W}$ 为投影矩阵，大小为 $d \\times k$。\n",
    "- $\\mathbf{Z}$ 为降维后的数据矩阵，大小为 $n \\times k$。\n",
    "\n",
    "### 解释：\n",
    "- 投影后的数据矩阵 $\\mathbf{Z}$ 是 $k$-维的，保留了数据中主要的变化方向。\n",
    "\n",
    "---\n",
    "\n",
    "## 数学推导核心\n",
    "\n",
    "PCA 的目标是寻找一组正交的投影方向，使得投影后数据的方差最大化。具体目标公式为：\n",
    "\n",
    "$$\n",
    "\\text{maximize: } \\mathbf{v}^\\top \\mathbf{C} \\mathbf{v} \\quad \\text{subject to: } \\|\\mathbf{v}\\| = 1\n",
    "$$\n",
    "\n",
    "解得的 $\\mathbf{v}$ 就是协方差矩阵的特征向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "某些特征的标准差为零，可能是由于数据中存在常数列",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 62\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28mprint\u001b[39m(Z)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 62\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 58\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m     57\u001b[0m     data \u001b[38;5;241m=\u001b[39m loadData(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msecom.data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 58\u001b[0m     Z \u001b[38;5;241m=\u001b[39m \u001b[43mpca\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28mprint\u001b[39m(Z)\n",
      "Cell \u001b[1;32mIn[13], line 26\u001b[0m, in \u001b[0;36mpca\u001b[1;34m(dataMat, nComponents)\u001b[0m\n\u001b[0;32m     24\u001b[0m std \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(dataMat, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(std \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m---> 26\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m某些特征的标准差为零，可能是由于数据中存在常数列\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m X \u001b[38;5;241m=\u001b[39m (dataMat \u001b[38;5;241m-\u001b[39m mean) \u001b[38;5;241m/\u001b[39m std\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# 2. 计算协方差矩阵\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: 某些特征的标准差为零，可能是由于数据中存在常数列"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def loadData(fileName):\n",
    "    with open(fileName, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        data = []\n",
    "        for row in reader:\n",
    "            data.append([float(x) for x in row[0].split()])\n",
    "    return np.array(data)\n",
    "\n",
    "def pca(dataMat, nComponents):\n",
    "    # 处理 NaN\n",
    "    imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    dataMat = imputer.fit_transform(dataMat)\n",
    "\n",
    "    # 检查是否还有 NaN\n",
    "    if np.isnan(dataMat).any():\n",
    "        raise ValueError(\"填充后仍存在 NaN 值，请检查数据\")\n",
    "\n",
    "    # 1. 数据标准化\n",
    "    mean = np.mean(dataMat, axis=0)\n",
    "    # std = np.std(dataMat, axis=0)\n",
    "    # if np.any(std == 0):\n",
    "    #     raise ValueError(\"某些特征的标准差为零，可能是由于数据中存在常数列\")\n",
    "    # X = (dataMat - mean) / std\n",
    "    X = (dataMat - mean)\n",
    "\n",
    "    # 2. 计算协方差矩阵\n",
    "    C = np.cov(X, rowvar=False)\n",
    "    if np.isnan(C).any() or np.isinf(C).any():\n",
    "        raise ValueError(\"协方差矩阵中存在 NaN 或无穷值\")\n",
    "\n",
    "    # 确保协方差矩阵是正定的\n",
    "    if not np.allclose(C, C.T):\n",
    "        raise ValueError(\"协方差矩阵不是对称的\")\n",
    "    C += np.eye(C.shape[0]) * 1e-8  # 数值稳定性调整\n",
    "\n",
    "    # 3. 计算特征值和特征向量\n",
    "    eigVals, eigVecs = np.linalg.eig(C)\n",
    "\n",
    "    # 4. 排序特征值和特征向量\n",
    "    idx = np.argsort(eigVals)[::-1]\n",
    "    eigVecs = eigVecs[:, idx]\n",
    "    eigVals = eigVals[idx]\n",
    "\n",
    "    # 5. 选择前 nComponents 个主成分\n",
    "    W = eigVecs[:, :nComponents]\n",
    "\n",
    "    # 6. 投影到主成分方向\n",
    "    Z = X @ W\n",
    "\n",
    "    return Z\n",
    "\n",
    "\n",
    "def main():\n",
    "    data = loadData(\"secom.data\")\n",
    "    Z = pca(data, 4)\n",
    "    print(Z)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Violet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
